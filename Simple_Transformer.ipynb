{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d043c9df-6cb3-4e87-aba9-b16951e3c529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: kafka-python in /home/matheus.zanoto/.local/lib/python3.10/site-packages (2.2.15)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/matheus.zanoto/.local/lib/python3.10/site-packages (2.8.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: jinja2 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: fsspec in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: networkx in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch) (59.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorboard in /home/matheus.zanoto/.local/lib/python3.10/site-packages (2.20.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from tensorboard) (2.2.6)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from tensorboard) (1.75.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from tensorboard) (9.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from tensorboard) (6.32.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard) (59.6.0)\n",
      "Requirement already satisfied: packaging in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from tensorboard) (25.0)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
      "Requirement already satisfied: filelock in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: requests in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.8/485.8 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 KB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Using cached regex-2025.9.18-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/matheus.zanoto/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed hf-xet-1.1.10 huggingface-hub-0.35.3 regex-2025.9.18 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.56.2\n"
     ]
    }
   ],
   "source": [
    "!pip install kafka-python\n",
    "!pip install torch\n",
    "!pip install tensorboard\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "711c0adb-4434-4a6c-aa7d-180ab8f54c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "61893402-4187-4efc-a65c-166fc6a041af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes para parametrizações de configuração no PubSub\n",
    "TOPIC = \"SIMPLE_TRANSFORMER_TOPIC\"\n",
    "\n",
    "# Constantes para parametrizações do modelo Transformer Simples\n",
    "VOCAB_SIZE = 30522  # Usando o vocabulário do BERT\n",
    "EMBED_SIZE = 3\n",
    "NUM_HEADS = 1\n",
    "NUM_STEPS = 1000\n",
    "NUM_PUBLISHER_BLOCKS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "47e01a50-561c-46ab-b73f-5e84639f8096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o modelo Transformer simples\n",
    "class SimpleTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads):\n",
    "        super(SimpleTransformerModel, self).__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)  # Embedding de 3 dimensões\n",
    "        self.attention = nn.MultiheadAttention(embed_size, num_heads)  # Camada de atenção multi-cabeça\n",
    "        self.fc = nn.Linear(embed_size, vocab_size)  # Camada de saída para predição de palavras\n",
    "\n",
    "        self.d_k = int(embed_size/num_heads)\n",
    "        self.n_heads = num_heads\n",
    "\n",
    "        # matrizes WQ, WK e WV que criaram as matrizes de\n",
    "        # vetores Q, K e V para nossas cabeças de atenção\n",
    "        self.Wq = nn.Linear(embed_size, embed_size)\n",
    "        self.Wk = nn.Linear(embed_size, embed_size)\n",
    "        self.Wv = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Aplica o embedding\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # Transpor para formato adequado para a camada de atenção\n",
    "        x = x.transpose(0, 1)  # A camada MultiheadAttention espera [seq_len, batch_size, embed_size]\n",
    "\n",
    "        # Gera as matrizes de vetores Q, K e V\n",
    "        Q = self.Wq(x)  # [seq_len, batch_size, embed_size]\n",
    "        K = self.Wk(x)  # [seq_len, batch_size, embed_size]\n",
    "        V = self.Wv(x)  # [seq_len, batch_size, embed_size]\n",
    "\n",
    "        # Passando Q, K, V pela camada de atenção\n",
    "        output, attention_weights = self.attention(Q, K, V)  # Aqui as entradas são as mesmas, mas isso pode ser alterado\n",
    "        attention_weights = torch.tensor([[attention_weights.tolist()]])\n",
    "        attention_output = output\n",
    "\n",
    "        # A camada de saída\n",
    "        output = self.fc(attention_output)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a98b78b6-0d4f-4b35-a88c-7d5b386805d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo classe para operações de treinamento usando o modelo de Transformer simples\n",
    "class SimpleTransformerTrainer():\n",
    "    def __init__(self, model, text_sentence_train, text_sentence_train_b):\n",
    "        super(SimpleTransformerTrainer, self).__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.text_sentence_train = text_sentence_train\n",
    "        self.text_sentence_train_b = text_sentence_train_b\n",
    "\n",
    "        # Inicializando o tokenizador\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        # Inicializando o otimizador\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def tokenize(self):\n",
    "        # Tokenizando o texto\n",
    "        inputs = self.tokenizer(self.text_sentence_train, self.text_sentence_train_b, return_tensors='pt')\n",
    "        self.input_ids = inputs['input_ids']  # IDs dos tokens\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        self.tokenize()\n",
    "        output, attention_weights = self.model(self.input_ids)  # Passa os dados pelo modelo\n",
    "\n",
    "        output = output.transpose(0, 1)  # Reverter a transposição\n",
    "        output = output.view(-1, self.model.vocab_size)  # Redimensionar para a forma correta\n",
    "        self.input_ids = self.input_ids.view(-1)  # Redimensionar para a forma correta\n",
    "        loss = self.criterion(output, self.input_ids)  # Calcular a perda\n",
    "\n",
    "        loss.backward()  # Retropropagar\n",
    "        self.optimizer.step()  # Atualizar os pesos\n",
    "        return loss.item(), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a616a655-dff3-46f3-bd20-bd30d95830ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PubSubPublisherClient():\n",
    "    def __init__(self, topic):\n",
    "        super(PubSubPublisherClient, self).__init__()\n",
    "\n",
    "        # Definindo configurações para conexão com o pubsub client usando Apache Kafka\n",
    "        self.topic = topic\n",
    "        \n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=[\"localhost:9092\"], # endereço do broker Kafka\n",
    "            value_serializer=lambda v: json.dumps(v).encode(\"utf-8\") # serialização em JSON\n",
    "        )\n",
    "\n",
    "    def build_payload(self, step, key_values):\n",
    "        payload = {\n",
    "            \"step\": step,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "\n",
    "        for key in key_values:\n",
    "            payload[key] = key_values[key]\n",
    "\n",
    "        return payload\n",
    "\n",
    "    def publish_update(self, payload):\n",
    "        self.producer.send(self.topic, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "174c9fba-f64a-4820-9b50-74b797c7df84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo classe para operações de treinamento conforme o Nº de iterações parametrizado pelo algoritmo, e comunicação com PubSub\n",
    "class SimpleTransformerTrainerManager():\n",
    "    def __init__(self, trainer, publisher, num_steps, num_publisher_blocks):\n",
    "        super(SimpleTransformerTrainerManager, self).__init__()\n",
    "\n",
    "        self.trainer = trainer\n",
    "        self.publisher = publisher\n",
    "        self.num_steps = num_steps\n",
    "        self.num_publisher_blocks = num_publisher_blocks\n",
    "\n",
    "        # Tensorboard\n",
    "        self.tb_logdir = \"runs/simple-transformer-training-metrics\"\n",
    "        self.writer = SummaryWriter(self.tb_logdir)\n",
    "\n",
    "    def add_scalar(self, key, value, step):\n",
    "        self.writer.add_scalar(key, value, step)\n",
    "\n",
    "    def train(self):\n",
    "        for step in range(1, self.num_steps+1):\n",
    "            loss, attention_weights = self.trainer.train()\n",
    "            \n",
    "            self.add_scalar(\"loss\", loss, step)\n",
    "\n",
    "            if step % self.num_publisher_blocks == 0:\n",
    "                key_values_payload = {\n",
    "                    \"loss\": loss,\n",
    "                    #\"attention_weights\": attention_weights.tolist()\n",
    "                }\n",
    "                payload = self.publisher.build_payload(step, key_values_payload)\n",
    "\n",
    "                self.publisher.publish_update(payload)\n",
    "\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        self.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b50f5f79-29d4-4899-979d-e942f25e08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformerModel(VOCAB_SIZE, EMBED_SIZE, NUM_HEADS)\n",
    "\n",
    "text_sentence_train = \"teste testando texto treinamento\"\n",
    "text_sentence_b = \"teste comparativo da outra sentenca\"\n",
    "\n",
    "trainer = SimpleTransformerTrainer(model, text_sentence_train, text_sentence_b)\n",
    "publisher = PubSubPublisherClient(TOPIC)\n",
    "trainer_manager = SimpleTransformerTrainerManager(trainer, publisher, NUM_STEPS, NUM_PUBLISHER_BLOCKS)\n",
    "trainer_manager.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "44ed403e-5678-45d8-b4d3-6f92511d8554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a0ace7c84bb4f8e4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a0ace7c84bb4f8e4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/simple-transformer-training-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3038ccb-2958-45ab-a06b-892c4dd3455e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
